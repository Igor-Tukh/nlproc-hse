{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSE natural language processing\n",
    "### HW 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_PATH = './resources'\n",
    "ODICT_FILENAME = 'odict.csv'\n",
    "OPCORPORA_FILENAME_XML = 'dict.opcorpora.xml'\n",
    "OPCORPORA_FILENAME = 'dict.opcorpora.txt'\n",
    "OPCORPORA_DUMP_FILENAME = 'opcorpora.pkl'\n",
    "TEST_FILENAME = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_TO_POS = defaultdict(lambda: 'NI')\n",
    "COMMENT_TO_POS['м'] = 'S'\n",
    "COMMENT_TO_POS['ж'] = 'S'\n",
    "COMMENT_TO_POS['с'] = 'S'\n",
    "COMMENT_TO_POS['со'] = 'S'\n",
    "COMMENT_TO_POS['жо'] = 'S'\n",
    "COMMENT_TO_POS['мо'] = 'S'\n",
    "COMMENT_TO_POS['мн'] = 'S'\n",
    "COMMENT_TO_POS['мо-жо'] = 'S'\n",
    "COMMENT_TO_POS['предл.'] = 'PR'\n",
    "COMMENT_TO_POS['п'] = 'A'\n",
    "COMMENT_TO_POS['межд.'] = 'ADV'\n",
    "COMMENT_TO_POS['предик.'] = 'ADV'\n",
    "COMMENT_TO_POS['н.'] = 'ADV'\n",
    "COMMENT_TO_POS['вводн.'] = 'ADV'\n",
    "COMMENT_TO_POS['сравн.'] = 'ADV'\n",
    "COMMENT_TO_POS['част.'] = 'ADV'\n",
    "COMMENT_TO_POS['св'] = 'V'\n",
    "COMMENT_TO_POS['нсв'] = 'V'\n",
    "COMMENT_TO_POS['св-нсв.'] = 'V'\n",
    "COMMENT_TO_POS['союз'] = 'CONJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = ['.', ',', '?', '!', os.linesep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_odict():\n",
    "    odict = {}\n",
    "    with open(os.path.join(RESOURCES_PATH, ODICT_FILENAME), encoding='cp1251') as csv_file:\n",
    "        dict_reader = csv.reader(csv_file)\n",
    "        for line in dict_reader:\n",
    "            pos = ''\n",
    "            lemma = line[0]\n",
    "            tag = COMMENT_TO_POS[line[1]]\n",
    "            odict_value = (lemma, tag)\n",
    "            odict[lemma] = odict_value\n",
    "            for token in line[2:]:\n",
    "                odict[token] = odict_value\n",
    "    return odict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "odict = load_odict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(test_input_path, test_output_path, word_dict):\n",
    "    with open(test_input_path, 'r') as input_file:\n",
    "        with open(test_output_path, 'w') as output_file :\n",
    "            for line in input_file.readlines():\n",
    "                for character in PUNCTUATION:\n",
    "                    line = line.replace(character, '')\n",
    "                tokens = []\n",
    "                for token in line.split(' '):\n",
    "                    if token == '':\n",
    "                        continue\n",
    "                    token_lower = token.lower()\n",
    "                    lemma, tag = (token_lower, 'NI')\n",
    "                    if token_lower in word_dict:\n",
    "                        lemma, tag = word_dict[token_lower]\n",
    "                    else:\n",
    "                        token_lower.replace('ё', 'е')\n",
    "                        if token_lower in word_dict:\n",
    "                            lemma, tag = word_dict[token_lower]\n",
    "                    tokens.append('{}{}{}={}{}'.format(token, '{', lemma, tag, '}'))\n",
    "                output_file.write(' '.join(tokens))\n",
    "                output_file.write(os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_task(os.path.join(RESOURCES_PATH, TEST_FILENAME + '.in'), os.path.join(RESOURCES_PATH, TEST_FILENAME + '.out'), odict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```You've got the score 0.7444979367262724 (88.5% correct lemmas and 76.1% correct POS tags)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC is out of memory when I am using ElementTree, so I will parse opcorpora dictionary manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grammemes_parents():\n",
    "    xml_path = os.path.join(RESOURCES_PATH, OPCORPORA_FILENAME_XML)\n",
    "    grammemes_parents = {}\n",
    "    grammeme_parent = None\n",
    "    for event, elem in ET.iterparse(xml_path, events=(\"start\", \"end\")):\n",
    "        if event == 'start':\n",
    "            if elem.tag == 'grammeme':\n",
    "                grammeme_parent = elem.attrib['parent']\n",
    "            elif elem.tag == 'name':\n",
    "                grammemes_parents[''.join([text for text in elem.itertext()])] = grammeme_parent\n",
    "        elif elem.tag == 'grammemes':\n",
    "            break\n",
    "    return grammemes_parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammemes_info = load_grammemes_parents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lemmas():\n",
    "    xml_path = os.path.join(RESOURCES_PATH, OPCORPORA_FILENAME_XML)\n",
    "    lemma_to_id = {}\n",
    "    id_to_lemma = {}\n",
    "    lemma_to_grammemes = {}\n",
    "    word_to_gramemmes = {}\n",
    "    word_to_lemma = {}\n",
    "    links = {}\n",
    "    lemma_id = None\n",
    "    lemma = None\n",
    "    word = None\n",
    "    in_l = False\n",
    "    in_f = False\n",
    "    for event, elem in ET.iterparse(xml_path, events=(\"start\", \"end\")):\n",
    "        if event == 'start':\n",
    "            if elem.tag == 'lemma':\n",
    "                lemma_id = elem.attrib['id']\n",
    "            elif elem.tag == 'l':\n",
    "                in_l = True\n",
    "                lemma = elem.attrib['t']\n",
    "                lemma_to_id[lemma] = lemma_id\n",
    "                id_to_lemma[lemma_id] = lemma\n",
    "            elif elem.tag == 'g':\n",
    "                if in_l:\n",
    "                    if lemma not in lemma_to_grammemes:\n",
    "                        lemma_to_grammemes[lemma] = []\n",
    "                    lemma_to_grammemes[lemma].append(elem.attrib['v'])\n",
    "                if in_f:\n",
    "                    if word not in word_to_gramemmes:\n",
    "                        word_to_gramemmes[word] = []\n",
    "                    word_to_gramemmes[word].append(elem.attrib['v'])\n",
    "            elif elem.tag == 'f':\n",
    "                in_f = True\n",
    "                word = elem.attrib['t']\n",
    "                word_to_lemma[word] = lemma\n",
    "            elif elem.tag == 'link':\n",
    "                l_from = elem.attrib['from']\n",
    "                l_to = elem.attrib['to']\n",
    "                links[l_to] = l_from\n",
    "        elif elem.tag == 'l':\n",
    "            in_l = False\n",
    "        elif elem.tag == 'f':\n",
    "            in_f = False\n",
    "    return lemma_to_id, id_to_lemma, lemma_to_grammemes, word_to_lemma, word_to_gramemmes, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_to_id, id_to_lemma, lemma_to_grammemes, word_to_lemma, word_to_gramemmes, links = load_lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "ADJF\n",
      "ADJS\n",
      "COMP\n",
      "VERB\n",
      "INFN\n",
      "PRTF\n",
      "PRTS\n",
      "GRND\n",
      "NUMR\n",
      "ADVB\n",
      "NPRO\n",
      "PRED\n",
      "PREP\n",
      "CONJ\n",
      "PRCL\n",
      "INTJ\n"
     ]
    }
   ],
   "source": [
    "for key, value in grammemes_info.items():\n",
    "    if value == 'POST':\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('долгий', 'A')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_word_opcorpora('долог')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMEME_TO_POS = defaultdict(lambda: 'NI')\n",
    "GRAMMEME_TO_POS['NOUN'] = 'S'\n",
    "GRAMMEME_TO_POS['ADJF'] = 'A'\n",
    "GRAMMEME_TO_POS['ADJS'] = 'A'\n",
    "GRAMMEME_TO_POS['COMP'] = 'A'\n",
    "GRAMMEME_TO_POS['VERB'] = 'V'\n",
    "GRAMMEME_TO_POS['INFN'] = 'V'\n",
    "GRAMMEME_TO_POS['PRTF'] = 'V'\n",
    "GRAMMEME_TO_POS['PRTS'] = 'V'\n",
    "GRAMMEME_TO_POS['GRND'] = 'V'\n",
    "GRAMMEME_TO_POS['ADVB'] = 'ADV'\n",
    "GRAMMEME_TO_POS['PRED'] = 'ADV'\n",
    "GRAMMEME_TO_POS['INTJ'] = 'ADV'\n",
    "GRAMMEME_TO_POS['PRCL'] = 'ADV'\n",
    "GRAMMEME_TO_POS['INTJ'] = 'ADV'\n",
    "GRAMMEME_TO_POS['PREP'] = 'PR'\n",
    "GRAMMEME_TO_POS['CONJ'] = 'CONJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONJS = ['a', 'благо', 'буде', 'будто', 'вдобавок', 'да', 'дабы', 'даже', 'же', 'едва', 'ежели', 'если', 'зато', \n",
    "         'зачем', 'и', 'ибо', 'или', 'кабы', 'как', 'когда', 'коли', 'либо', 'лишь', 'нежели', 'но', 'однако',\n",
    "         'однако', 'особенно', 'оттого', 'отчего', 'пока', 'покуда', 'поскольку', 'потому', 'почему', 'притом',\n",
    "         'причем', 'пускай', 'пусть', 'раз', 'словно', 'также', 'тоже', 'только', 'точно', 'хотя', 'чем', 'что',\n",
    "         'чтоб', 'чтобы']\n",
    "CONJS = set(CONJS)\n",
    "\n",
    "PRS = ['а-ля', 'без', 'безо', 'благодаря', 'близ', 'в', 'вблизи', 'ввиду', 'вглубь', 'вдогон', 'вдоль', 'взамен',\n",
    "       'включая', 'вкруг', 'вместо', 'вне', 'внизу', 'внутри', 'внутрь', 'во', 'вовнутрь', 'возле', 'вокруг',\n",
    "       'вопреки', 'вразрез', 'вроде', 'вслед', 'вследствие', 'для', 'для-ради', 'до', 'за', 'замест', 'заместо',\n",
    "       'из', 'из-за', 'из-под', 'из-подо', 'внутри', 'изо', 'исключая', 'к', 'касаемо', 'касательно', 'ко',\n",
    "       'кроме', 'кругом', 'меж', 'между', 'мимо', 'на', 'наверху', 'навроде', 'навстречу', 'над', 'надо', 'назад',\n",
    "       'назади', 'назло', 'накануне', 'наместо', 'наперекор', 'наперерез', 'наперехват', 'наподобие', 'наподобье',\n",
    "       'напротив', 'наряду', 'насупротив', 'насчет', 'несмотря', 'ниже', 'о', 'об', 'обо', 'обок', 'обочь', \n",
    "       'около', 'окрест', 'окроме', 'окромя', 'округ', 'опосля', 'опричь', 'от', 'ото', 'перед', 'передо',\n",
    "       'по', 'повдоль', 'поверх', 'под', 'подле', 'подо', 'подобно', 'позади', 'помимо', 'поперёд', 'поперёк',\n",
    "       'порядка', 'посереди', 'посередине', 'посредь', 'после', 'посреди', 'посредине', 'посредством', 'пред',\n",
    "       'предо', 'прежде', 'при', 'про', 'промеж', 'помежду', 'против', 'ради', 'с', 'со', 'сверх', 'сверху']\n",
    "\n",
    "PRS = set(PRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grammeme(grammeme):\n",
    "    pos = None\n",
    "    while grammeme in grammemes_info:\n",
    "        pr = grammemes_info[grammeme]\n",
    "        if pr == 'POST':\n",
    "            pos = pr\n",
    "            break\n",
    "        elif pr == '':\n",
    "            break\n",
    "        grammeme = pr\n",
    "    if not pos is None:\n",
    "        return grammeme\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_opcorpora(word):\n",
    "    grammemes = []\n",
    "    for gr in lemma_to_grammemes[word]:\n",
    "        grammeme = process_grammeme(gr)\n",
    "        if not grammeme is None and not GRAMMEME_TO_POS[gr] == 'NI':\n",
    "            grammemes.append(GRAMMEME_TO_POS[gr])\n",
    "    if len(grammemes) == 0:\n",
    "        return 'NI'\n",
    "    return grammemes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_opcorpora(word):\n",
    "    word = word.lower()\n",
    "    word_mod = word.replace('ё', 'е')\n",
    "    if word not in word_to_lemma:\n",
    "        if word != word_mod:\n",
    "            return process_word_opcorpora(word_mod)\n",
    "        return word, 'NI'\n",
    "    lemma = word_to_lemma[word]\n",
    "    while lemma_to_id[lemma] in links:\n",
    "        lemma_new = id_to_lemma[links[lemma_to_id[lemma]]]\n",
    "        if lemma_new == lemma:\n",
    "            break\n",
    "        lemma = lemma_new\n",
    "        break\n",
    "    return lemma, get_word_pos_opcorpora(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_odict(word):\n",
    "    word = word.lower()\n",
    "    lemma, tag = (word, 'NI')\n",
    "    if word in odict:\n",
    "        lemma, tag = odict[word]\n",
    "    else:\n",
    "        word.replace('ё', 'е')\n",
    "        if word in odict:\n",
    "            lemma, tag = odict[word]\n",
    "    return lemma, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task_both(test_input_path, test_output_path):\n",
    "    with open(test_input_path, 'r') as input_file:\n",
    "        with open(test_output_path, 'w') as output_file :\n",
    "            for line in tqdm(input_file.readlines()):\n",
    "                for character in PUNCTUATION:\n",
    "                    line = line.replace(character, '')\n",
    "                tokens = []\n",
    "                for ind, token in enumerate(line.split(' ')):\n",
    "                    if token == '':\n",
    "                        continue\n",
    "                    lemma, tag = process_word_opcorpora(token)\n",
    "                    if tag == 'NI':\n",
    "                        tag = process_word_odict(token)[1]\n",
    "                    if not token[0].islower() and ind != 0:\n",
    "                        lemma = lemma[0].upper() + lemma[1:]\n",
    "                    if token.lower() in CONJS:\n",
    "                        lemma, tag = token.lower(), 'CONJ'\n",
    "                    if token.lower() in PRS:\n",
    "                        lemma, tag = token.lower(), 'PR'\n",
    "                    if tag == 'NI':\n",
    "                        tag = 'ADV'\n",
    "                    tokens.append('{}{}{}={}{}'.format(token, '{', lemma, tag, '}'))\n",
    "                output_file.write(' '.join(tokens))\n",
    "                output_file.write(os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ae3bce70f24aa8a1acaf9032f1c56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task_both(os.path.join(RESOURCES_PATH, TEST_FILENAME + '.in'), os.path.join(RESOURCES_PATH, TEST_FILENAME + '.out'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
