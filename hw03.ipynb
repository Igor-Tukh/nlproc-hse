{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSE natural language processing\n",
    "### HW 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_PATH = './resources'\n",
    "TEST_IN_FILENAME = os.path.join(RESOURCES_PATH, 'test.in')\n",
    "TEST_OUT_FILENAME = os.path.join(RESOURCES_PATH, 'test.out')\n",
    "TEST_TST_FILENAME = os.path.join(RESOURCES_PATH, 'test.tst')\n",
    "TRAIN_SENTENCES_FILENAME = os.path.join(RESOURCES_PATH, 'train_sentences.txt')\n",
    "TRAIN_TAGS_FILE = os.path.join(RESOURCES_PATH, 'train_nes.txt')\n",
    "DEVSET_DIR = os.path.join(RESOURCES_PATH, 'factRuEval-2016', 'devset')\n",
    "NE3_DIR = os.path.join(RESOURCES_PATH, 'ne3', 'Collection3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_set = set(string.punctuation)\n",
    "punct_set.add(os.linesep)\n",
    "punct_set.add('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    try:\n",
    "        lemmas = m.lemmatize(sentence)\n",
    "        process_lemma = lambda lemma: ' ' if lemma == ' ' else lemma.replace(' ', '') if lemma.replace(' ', '') not in punct_set else ' '\n",
    "        return ''.join([process_lemma(lemma) for lemma in lemmas if lemma not in punct_set]).replace(os.linesep, '')\n",
    "    except:\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(lemmatize_dict):\n",
    "    output = defaultdict(lambda: [])\n",
    "    \n",
    "    with open(TEST_IN_FILENAME, 'r') as in_file:\n",
    "        with open(TEST_OUT_FILENAME, 'w') as out_file:            \n",
    "            for line_ind, line in tqdm(enumerate(in_file.readlines())):\n",
    "                n = len(line)\n",
    "                ind = 0\n",
    "                tokens = []\n",
    "                while ind < n:\n",
    "                    while ind < n and not line[ind].isalpha():\n",
    "                        ind += 1\n",
    "                    if ind >= n:\n",
    "                        break\n",
    "                    next_ind = ind\n",
    "                    while next_ind < n and line[next_ind].isalpha():\n",
    "                        next_ind += 1\n",
    "                    tokens.append((ind, next_ind))\n",
    "                    ind = next_ind\n",
    "                token_ind = 0\n",
    "                while token_ind < len(tokens):\n",
    "                    marked = False\n",
    "                    next_token_ind = token_ind\n",
    "                    wf, wt = tokens[token_ind]\n",
    "                    last_good_next_token = -1\n",
    "                    while next_token_ind < min(token_ind + 8, len(tokens)):\n",
    "                        current_word = lemmatize_sentence(line[wf:tokens[next_token_ind][1]])\n",
    "                        if current_word in lemmatize_dict:\n",
    "                            marked = True\n",
    "                            last_good_next_token = next_token_ind\n",
    "                        next_token_ind += 1\n",
    "                    if not marked:\n",
    "                        word = line[tokens[token_ind][0]:tokens[token_ind][1]]\n",
    "                        try:\n",
    "                            grammemas = set(m.analyze(word)[0]['analysis'][0]['gr'].split(','))\n",
    "                        except:\n",
    "                            grammemas = set()\n",
    "                        if 'фам' in grammemas or 'имя' in grammemas:\n",
    "                            output[line_ind].append((tokens[token_ind][0], tokens[token_ind][1] - tokens[token_ind][0], 'PERSON'))\n",
    "                            out_file.write('{} {} PERSON '.format(tokens[token_ind][0], tokens[token_ind][1] - tokens[token_ind][0]))\n",
    "                        token_ind += 1\n",
    "                    else:\n",
    "                        current_word = lemmatize_sentence(line[wf:tokens[last_good_next_token][1]])\n",
    "                        all_tags = lemmatize_dict[current_word]\n",
    "                        for ind in range(token_ind, last_good_next_token + 1):\n",
    "                            for tag in all_tags:\n",
    "                                output[line_ind].append((tokens[ind][0], tokens[ind][1] - tokens[ind][0], tag))\n",
    "                                out_file.write('{} {} {} '.format(tokens[ind][0], tokens[ind][1] - tokens[ind][0], tag))\n",
    "                        token_ind = last_good_next_token + 1\n",
    "                            \n",
    "                \n",
    "                out_file.write('EOL')\n",
    "                out_file.write(os.linesep)\n",
    "                \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_dict():\n",
    "    train_dict = {}\n",
    "    with open(TRAIN_SENTENCES_FILENAME, 'r') as train_sentences_file:\n",
    "        with open(TRAIN_TAGS_FILE, 'r') as train_tags_file:\n",
    "            for tag_line, sentence in zip(train_tags_file.readlines(), train_sentences_file.readlines()):\n",
    "                tags = tag_line.split(' ')[:-1]\n",
    "                last_token = ''\n",
    "                tokens = [(int(tags[ind]), int(tags[ind]) + int(tags[ind + 1]), tags[ind + 2]) for ind in range(0, len(tags), 3)]\n",
    "                tokens.sort(key=itemgetter(2, 0))\n",
    "                for ind, token in enumerate(tokens):\n",
    "                    word_from, word_to, tag = token\n",
    "                    next_from = -1 if ind == len(tokens) - 1 else tokens[ind + 1][0]\n",
    "                    next_tag = '' if ind == len(tokens) - 1 else tokens[ind + 1][2]\n",
    "                    word = sentence[word_from: word_to]\n",
    "                    if ind + 3 < len(tags) and next_from - word_to <= 5:\n",
    "                        all_not_alpha = True\n",
    "                        for letter in sentence[word_to:next_from]:\n",
    "                            all_not_alpha = all_not_alpha and not letter.isalpha() and letter != ',' and letter != '(' and letter != ')'\n",
    "                        if all_not_alpha:\n",
    "                            last_token += sentence[word_from:next_from]\n",
    "                            continue\n",
    "                    word = last_token + word\n",
    "                    if word not in train_dict:\n",
    "                        train_dict[word] = []\n",
    "                    train_dict[word].append(tag)\n",
    "                    last_token = ''\n",
    "    \n",
    "    lemmatize_train_dict = {}\n",
    "    for key in train_dict.keys():\n",
    "        lemmatize_train_dict[lemmatize_sentence(key)] = list(set(train_dict[key]))\n",
    "    return lemmatize_train_dict, train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_train_dict, train_dict = load_train_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1594e150559945408bba5d43d914df6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(lemmatize_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\".*\\.objects\")\n",
    "OBJECT_FILENAMES = [os.path.join(DEVSET_DIR, filename) for filename in os.listdir(DEVSET_DIR) if pattern.match(filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_object_files():\n",
    "    tag_to_real_tag = {'LocOrg' : 'ORG', 'Org' : 'ORG', 'Person' : 'PERSON'}\n",
    "    lemmatize_dict = {}\n",
    "    for filename in OBJECT_FILENAMES:\n",
    "        with open(filename, 'r') as object_file:\n",
    "            for line in object_file.readlines():\n",
    "                tokens = line.split(' ')\n",
    "                tag = tokens[1]\n",
    "                word = ''\n",
    "                for token_ind in range(2, len(tokens)):\n",
    "                    if tokens[token_ind] == '#':\n",
    "                        word = lemmatize_sentence(' '.join(tokens[token_ind + 1:]).replace(os.linesep, ''))\n",
    "                        if tag not in tag_to_real_tag:\n",
    "                            continue\n",
    "                        tag = tag_to_real_tag[tag]\n",
    "                        if word not in lemmatize_dict:\n",
    "                            lemmatize_dict[word] = []\n",
    "                        lemmatize_dict[word].append(tag)\n",
    "                        break\n",
    "    return lemmatize_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_files_dict = parse_object_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in object_files_dict.keys():\n",
    "    if key not in lemmatize_train_dict:\n",
    "        lemmatize_train_dict[key] = object_files_dict[key]\n",
    "    else:\n",
    "        lemmatize_train_dict[key] = list(set(object_files_dict[key] + lemmatize_train_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640f457fee4c4dbdabc7f71929df0952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(lemmatize_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\".*\\.ann\")\n",
    "NE3_FILENAMES = [os.path.join(NE3_DIR, filename) for filename in os.listdir(NE3_DIR) if pattern.match(filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ne3_files():\n",
    "    lemmatize_dict = {}\n",
    "    for filename in NE3_FILENAMES:\n",
    "        with open(filename, 'r') as ne3_file:\n",
    "            tag_to_real_tag = {'ORG' : 'ORG', 'PER' : 'PERSON'}\n",
    "            for line in ne3_file.readlines():\n",
    "                line = line.replace('\\t', ' ').replace('\\n', '')\n",
    "                tokens = line.split(' ')\n",
    "                tag = tokens[1]\n",
    "                word = ''\n",
    "                for token_ind in range(2, len(tokens)):\n",
    "                    if tokens[token_ind].isalpha():\n",
    "                        word = lemmatize_sentence(' '.join(tokens[token_ind:]).replace(os.linesep, ''))\n",
    "                        if tag not in tag_to_real_tag:\n",
    "                            break\n",
    "                        tag = tag_to_real_tag[tag]\n",
    "                        if word not in lemmatize_dict:\n",
    "                            lemmatize_dict[word] = []\n",
    "                        lemmatize_dict[word].append(tag)\n",
    "                        break\n",
    "    return lemmatize_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne3_files_dict = parse_ne3_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ne3_files_dict.keys():\n",
    "    if key not in lemmatize_train_dict:\n",
    "        lemmatize_train_dict[key] = ne3_files_dict[key]\n",
    "    else:\n",
    "        lemmatize_train_dict[key] = list(set(ne3_files_dict[key] + lemmatize_train_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b95c2cb697b4fecb6f1353154de07ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(lemmatize_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(lemmatize_train_dict.keys())\n",
    "for key in keys:\n",
    "    if '(' in key:\n",
    "        tokens = key.split('(')\n",
    "        tokens[1] = tokens[1].split(')')[0]\n",
    "        for token in [tokens[0], tokens[1]]:\n",
    "            if token not in lemmatize_train_dict:\n",
    "                lemmatize_train_dict[token] = []\n",
    "            lemmatize_train_dict[token] += lemmatize_train_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in lemmatize_train_dict.keys():\n",
    "    lemmatize_train_dict[key] = list(set(lemmatize_train_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea6333bbe9a46dfbadd0f1e559a9205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(lemmatize_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(lemmatize_train_dict.keys())\n",
    "for key in keys:\n",
    "    if '\"' in key:\n",
    "        for token in key.split('\"'):\n",
    "            if token == os.linesep:\n",
    "                continue\n",
    "            if token not in lemmatize_train_dict:\n",
    "                lemmatize_train_dict[token] = []\n",
    "            lemmatize_train_dict[token] += lemmatize_train_dict[key]\n",
    "            \n",
    "for key in lemmatize_train_dict.keys():\n",
    "    lemmatize_train_dict[key] = list(set(lemmatize_train_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc80666b39542649f0e3359a011f6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(lemmatize_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(lemmatize_train_dict.keys())\n",
    "for key in keys:\n",
    "    if '\\'' or '\"' in key:\n",
    "        for token in key.replace('\\'', '\"').split('\"'):\n",
    "            token = lemmatize_sentence(token).replace(os.linesep, '')\n",
    "            if token == '':\n",
    "                continue\n",
    "            if token not in lemmatize_train_dict:\n",
    "                lemmatize_train_dict[token] = []\n",
    "            lemmatize_train_dict[token] += lemmatize_train_dict[key]\n",
    "            \n",
    "for key in lemmatize_train_dict.keys():\n",
    "    lemmatize_train_dict[key] = list(set(lemmatize_train_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4220d0987a2541ca8fcc4141c9e0fc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(lemmatize_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(lemmatize_train_dict.keys())\n",
    "for key in keys:\n",
    "    if '<' in key or '>' in key:\n",
    "        for token in key.replace('<', '>').split('>'):\n",
    "            token = lemmatize_sentence(token).replace(os.linesep, '')\n",
    "            if token == '':\n",
    "                continue\n",
    "            if token not in lemmatize_train_dict:\n",
    "                lemmatize_train_dict[token] = []\n",
    "            lemmatize_train_dict[token] += lemmatize_train_dict[key]\n",
    "            \n",
    "for key in lemmatize_train_dict.keys():\n",
    "    lemmatize_train_dict[key] = list(set(lemmatize_train_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2081ce42f144bfcac3b16a6efe61a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = run_task(lemmatize_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_tags(output):\n",
    "    with open(TEST_IN_FILENAME, 'r') as in_file:\n",
    "        with open(TEST_TST_FILENAME, 'w') as out_file:            \n",
    "            for line_ind, line in tqdm(enumerate(in_file.readlines())):\n",
    "                prev = 0\n",
    "                for wf, wl, tag in output[line_ind]:\n",
    "                    out_file.write(line[prev:wf])\n",
    "                    prev = wf + wl\n",
    "                    out_file.write('{}'.format(tag))\n",
    "                out_file.write(line[prev:])\n",
    "                out_file.write(os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67339fe527a74f319ec9751d4110ff6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ced7556016149588c6eaf43bf62f9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Single '}' encountered in format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-57fefefca3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubstitute_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_train_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-85fc3f011ba3>\u001b[0m in \u001b[0;36msubstitute_tags\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\{{}\\}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinesep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Single '}' encountered in format string"
     ]
    }
   ],
   "source": [
    "substitute_tags(run_task(lemmatize_train_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
