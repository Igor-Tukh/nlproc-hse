{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSE natural language processing\n",
    "### HW 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_PATH = './resources'\n",
    "TRAIN_TEXTS_PATH = os.path.join(RESOURCES_PATH, 'texts_train.txt')\n",
    "TRAIN_SCORES_PATH = os.path.join(RESOURCES_PATH, 'scores_train.txt')\n",
    "TEST_INPUT_FILENAME = os.path.join(RESOURCES_PATH, 'test.in')\n",
    "TEST_OUTPUT_FILENAME = os.path.join(RESOURCES_PATH, 'test.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores():\n",
    "    with open(TRAIN_SCORES_PATH, 'r') as scores_file:\n",
    "        return np.array(list(map(int, scores_file.readlines())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts():\n",
    "    with open(TRAIN_TEXTS_PATH, 'r') as texts_file:\n",
    "        return np.array(texts_file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(evaluator):\n",
    "    with open(TEST_INPUT_FILENAME, 'r') as input_file:\n",
    "        with open(TEST_OUTPUT_FILENAME, 'w') as output_file:\n",
    "            for line in input_file.readlines():\n",
    "                output_file.write(evaluator(line))\n",
    "                output_file.write(os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base approach: average mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mark: 8\n"
     ]
    }
   ],
   "source": [
    "avg_mark = int(round(load_scores().mean()))\n",
    "print('Average mark: {}'.format(avg_mark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_task(lambda sentence: str(avg_mark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple approach: avreage unigram score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line_to_tokens(line):\n",
    "    return [token for token in re.split('\\W+', line.lower().replace(os.linesep, '')) if token != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543fa8052a0b4f05adf140b2a7c58093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "marks = defaultdict(lambda: [])\n",
    "last = defaultdict(lambda: -1)\n",
    "\n",
    "for ind, (text, score) in tqdm(enumerate(zip(texts, scores))):\n",
    "    for token in parse_line_to_tokens(text):\n",
    "        if last[token] != ind:\n",
    "            marks[token].append(score)\n",
    "            last[token] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mark = {token : 1. * sum(marks[token]) / len(marks[token]) for token in marks.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_mark(word):\n",
    "    return average_mark[word] if word in average_mark else avg_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_average_mark(line):\n",
    "    return str(int(round(np.array([get_average_mark(token) for token in parse_line_to_tokens(line)]).mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_task(get_text_average_mark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
